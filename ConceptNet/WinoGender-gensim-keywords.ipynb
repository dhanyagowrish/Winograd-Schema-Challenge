{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "from numpy import array\n",
    "import statistics\n",
    "import csv\n",
    "from gensim.summarization import keywords\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a stopwords list\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.extend(['participant','nom_pronoun','occupation','$','acc_pronoun','poss_pronoun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading WinoGender dataset\n",
    "\n",
    "wGender=open('datasets\\WinoGender.tsv')\n",
    "wg=csv.reader(wGender,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns relatedness value between two words - word1 & word2\n",
    "\n",
    "def relatedness(word1,word2):\n",
    "    link='http://api.conceptnet.io/relatedness?node1=/c/en/' + word1 + '&node2=/c/en/' + word2\n",
    "    relatedness_obj=requests.get(link).json()\n",
    "    val=relatedness_obj['value']\n",
    "    \n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature vector for a sentence \n",
    "# each component is the relatedness value between \n",
    "\n",
    "def extract_feature(sentence_tokens,occupation,participant):\n",
    "    \n",
    "    #print(sentence_tokens)\n",
    "    k1=sentence_tokens[0]\n",
    "    k2=sentence_tokens[1]\n",
    "        \n",
    "    ftr1 = relatedness(occupation, k1)\n",
    "\n",
    "    ftr2 = relatedness(occupation, k2)\n",
    "\n",
    "    ftr3 = relatedness(participant, k1)\n",
    "    \n",
    "    ftr4 = relatedness(participant, k2)\n",
    "\n",
    "    return [ftr1,ftr2,ftr3,ftr4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main + read input data\n",
    "if __name__ == '__main__':    \n",
    "    #list of all occupations\n",
    "    occupations=[]\n",
    "\n",
    "    #list of all participants\n",
    "    participants=[]\n",
    "\n",
    "    #tokenized_sentences\n",
    "    sentence_tokens=[]\n",
    "\n",
    "    #target answers\n",
    "    answers=[]\n",
    "\n",
    "    #whole sentences\n",
    "    sentences=[]\n",
    "\n",
    "    # for every schema in dataset extract occupation, participant, true answer and keywords\n",
    "    for row in wg:\n",
    "        text=row[3].lower()\n",
    "        text_tokens=word_tokenize(text)\n",
    "\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "        my_sentence=\" \".join(tokens_without_sw)\n",
    "\n",
    "        # extract two keywords from schema\n",
    "        ans=keywords(my_sentence,words=2,split=False,lemmatize = True)\n",
    "        li = ans.split(\" \")\n",
    "       \n",
    "\n",
    "        req=1\n",
    "        if(len(li))==1:\n",
    "            if '' in li:\n",
    "                req=0\n",
    "            else:\n",
    "                for ele in li:\n",
    "                    if \"\\n\" in ele:\n",
    "                        li=ele.split(\"\\n\")\n",
    "\n",
    "        if(len(li))==1:\n",
    "            reqd=0\n",
    "\n",
    "        if len(li)==2:\n",
    "            sentence_tokens.append(li)\n",
    "            occupations.append(row[0])\n",
    "            participants.append(row[1])\n",
    "            if(row[2]=='1'):\n",
    "                answers.append(1)\n",
    "            else:\n",
    "                answers.append(-1)\n",
    "\n",
    "            text=text.replace(\"$occupation\",row[0].lower())\n",
    "            text=text.replace(\"$participant\",row[1].lower())\n",
    "            sentences.append(text)\n",
    "\n",
    "\n",
    "    occupations=occupations[1:]\n",
    "    participants=participants[1:]\n",
    "    answers=answers[1:]\n",
    "    sentence_tokens=sentence_tokens[1:]\n",
    "    sentences=sentences[1:]\n",
    "\n",
    "    print(\"==== WINOGENDER SCHEMAS -> TRAIN: 88 VAL : 32 ====\")\n",
    "    \n",
    "    \n",
    "    print(\"Start training the model...\")\n",
    "\n",
    "    total = len(sentences)\n",
    "    train_size = 88\n",
    "    train_feature = []\n",
    "    train_target=[]\n",
    "    \n",
    "    train_count=0\n",
    "    train_sentences_present=[]\n",
    "    train_sentences_absent=[]\n",
    "    \n",
    "    for i in range(train_size):\n",
    "        sent_tokens=sentence_tokens[i]\n",
    "        occ=occupations[i]\n",
    "        part=participants[i]\n",
    "        try:\n",
    "            feature = extract_feature(sent_tokens,occ,part)\n",
    "            train_target.append(answers[i])\n",
    "            train_feature.append(feature)\n",
    "            train_count+=1\n",
    "            train_sentences_present.append(sentences[i])\n",
    "\n",
    "        except:\n",
    "            train_sentences_absent.append(sentences[i])\n",
    "            \n",
    "            \n",
    "\n",
    "    clf = svm.SVC(gamma=\"scale\")\n",
    "    clf.fit(train_feature, train_target)\n",
    "\n",
    "\n",
    "\n",
    "    test_size = total - train_size\n",
    "    test_feature = []\n",
    "    \n",
    "    val_sentences_present=[]\n",
    "    val_sentences_absent=[]\n",
    "    val_count=0\n",
    "    \n",
    "    val_target_answers=[]\n",
    "   \n",
    "    for i in range(test_size):\n",
    "        sent_tokens=sentence_tokens[i+train_size]\n",
    "        occ=occupations[i+train_size]\n",
    "        part=participants[i+train_size]\n",
    "        try:\n",
    "            feature = extract_feature(sent_tokens,occ,part)\n",
    "            val_target_answers.append(answers[i+train_size])\n",
    "            test_feature.append(feature)\n",
    "            val_count+=1\n",
    "            val_sentences_present.append(sentences[i+train_size])\n",
    "           \n",
    "        except:\n",
    "            val_sentences_absent.append(sentences[i+train_size])\n",
    "        \n",
    "            \n",
    "    test_answer = clf.predict(test_feature)\n",
    "    \n",
    "            \n",
    "    val_correct_sent=[]\n",
    "    val_wrong_sent=[]\n",
    "\n",
    "    print(\"Start caculating accuracy...\")\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_answer)):\n",
    "        if test_answer[i] == val_target_answers[i]:\n",
    "            correct += 1\n",
    "            val_correct_sent.append(val_sentences_present[i])\n",
    "        else:\n",
    "            val_wrong_sent.append(val_sentences_present[i])\n",
    "\n",
    "    print('Accuracy of SVM:', round(float(correct) / len(test_answer) * 100,3),  \"%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
