{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "from numpy import array\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with vocabulary consisting of top 400,000 word vectors\n",
    "model=KeyedVectors.load_word2vec_format(r\"\\GoogleNews-vectors-negative300.bin\",binary=True,limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining tags for different Parts-Of-Speech\n",
    "VERBS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "ADJECTIVE = ['JJ', 'JJR', 'JJS']\n",
    "NOUN=['NN','NNP','NNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return tags for a sentence\n",
    "def make_tags(sentence):\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    return nltk.pos_tag(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return if a word is a verb\n",
    "def is_verb(tup):\n",
    "    for v in VERBS:\n",
    "        if tup[1] == v:\n",
    "            return v\n",
    "    return False\n",
    "\n",
    "# return if a word is a noun\n",
    "def is_noun(tup):\n",
    "    for n in NOUN:\n",
    "        if tup[1] == n:\n",
    "            return n\n",
    "    return False\n",
    "\n",
    "# return if a word is adjective\n",
    "def is_adj(tup):\n",
    "    for a in ADJECTIVE:\n",
    "        if tup[1] == a:\n",
    "            return a\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature vector for schema\n",
    "\n",
    "def extract_feature(sentence1, sentence2, candidate1, candidate2,c_1,c_2):\n",
    "\n",
    "    sent1_tags = make_tags(sentence1)\n",
    "    sent2_tags = make_tags(sentence2)\n",
    "    tags=sent1_tags+sent2_tags\n",
    "    \n",
    "    li=[]\n",
    "    li.extend(c_1.strip(\" \")+c_2.strip(\" \"))\n",
    "    \n",
    "    sentence_tokens=[word for word in tags if word[0] not in li]\n",
    "\n",
    "    sent_verb=[]\n",
    "    sent_adj=[]\n",
    "    sent_noun=[]\n",
    "    \n",
    "    for v in sentence_tokens[::-1]:\n",
    "        if is_verb(v):\n",
    "            sent_verb.append(v[0])\n",
    "            \n",
    "    for adj in sentence_tokens[::-1]:\n",
    "        if is_adj(adj):\n",
    "            sent_adj.append(adj[0])\n",
    "            \n",
    "\n",
    "    for v in sentence_tokens[::-1]:\n",
    "        if is_noun(v):\n",
    "            sent_noun.append(v[0])\n",
    "           \n",
    "    final_keywords=[]\n",
    "    \n",
    "    while(len(final_keywords)<2):\n",
    "        \n",
    "        if(len(sent_noun)>0):\n",
    "            key_noun =  sent_noun[0]\n",
    "            final_keywords.append(key_noun)\n",
    "            sent_noun.pop(0)\n",
    "        \n",
    "        if(len(sent_verb)>0):\n",
    "            key_verb  = sent_verb[0]\n",
    "            final_keywords.append(key_verb)\n",
    "            sent_verb.pop(0)\n",
    "                \n",
    "        if(len(final_keywords)>=2):\n",
    "            break\n",
    "            \n",
    "        if(len(sent_adj)>0):\n",
    "            key_adj=sent_adj[0]\n",
    "            final_keywords.append(key_adj)\n",
    "            sent_adj.pop(0)\n",
    "            \n",
    "    k_1=final_keywords[0]\n",
    "    k_2=final_keywords[1]\n",
    "  \n",
    "    ftr1 = model.similarity(candidate1, k_1)\n",
    "\n",
    "    ftr2 = model.similarity(candidate1, k_2)\n",
    "\n",
    "    ftr3 = model.similarity(candidate2, k_1)\n",
    "    \n",
    "    ftr4 = model.similarity(candidate2, k_2)\n",
    "    \n",
    "\n",
    "    return [ftr1,ftr2,ftr3,ftr4]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"==== 80:20 TRAIN:VAL ...TRAIN=228 VAL=57 SCHEMAS====\")\n",
    "\n",
    "    sentences = []\n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    conjs = []\n",
    "    prons = []\n",
    "    answer = []\n",
    "    \n",
    "    candidates=[]\n",
    "    \n",
    "    candidate1=[]\n",
    "    candidate2=[]\n",
    "\n",
    "    \n",
    "    filepath = \"datasets\\WSC-285.xml\"\n",
    "    \n",
    "\n",
    "    xml_data = xml.etree.ElementTree.parse(filepath).getroot()\n",
    "\n",
    "    size = 0\n",
    "\n",
    "    for schema in xml_data.findall('schema'):\n",
    "\n",
    "        sent1.append(schema[0][0].text.lower().strip())\n",
    "        sent2.append(schema[0][2].text.lower().strip())\n",
    "        prons.append(schema[0][1].text)\n",
    "     \n",
    "        mylist=[]\n",
    "        for i in range(0,2):\n",
    "            cand=schema[2][i].text.lower()\n",
    "            cand_text_tokens = word_tokenize(cand)\n",
    "            if \"the\" in cand_text_tokens:\n",
    "                cand_text_tokens.remove(\"the\")\n",
    "            mylist.append(cand_text_tokens[0])\n",
    "            \n",
    "        candidates.append(mylist)\n",
    "        \n",
    "        c_1=schema[2][0].text.lower()\n",
    "        c_2=schema[2][1].text.lower()\n",
    "        candidate1.append(c_1)\n",
    "        candidate2.append(c_2)\n",
    "\n",
    "        ans = schema[3].text.strip()\n",
    "        if ans == 'A':\n",
    "            answer.append(1)\n",
    "        else:\n",
    "            answer.append(-1)\n",
    "\n",
    "        sentences.append(sent1[size] + ' ' + prons[size] + ' ' + sent2[size])\n",
    "\n",
    "        size += 1\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "\n",
    "    total = len(sentences)\n",
    "    train_size = 228\n",
    "    train_feature = []\n",
    "    train_target=[]\n",
    "    \n",
    "    train_count=0\n",
    "    train_sentences_present=[]\n",
    "    train_sentences_absent=[]\n",
    "\n",
    "    \n",
    "    for i in range(train_size):\n",
    "        s1 = sent1[i]\n",
    "        s2 = sent2[i]\n",
    "        can1=candidates[i][0]\n",
    "        can2=candidates[i][1]\n",
    "        try:\n",
    "            feature = extract_feature(s1, s2,candidates[i][0],candidates[i][1],candidate1[i],candidate2[i])\n",
    "            train_target.append(answer[i])\n",
    "            train_feature.append(feature)\n",
    "            train_count+=1\n",
    "            train_sentences_present.append(sentences[i])\n",
    "\n",
    "        except:\n",
    "            train_sentences_absent.append(sentences[i])\n",
    "            \n",
    "            \n",
    "\n",
    "    clf = svm.SVC(gamma=\"scale\")\n",
    "    clf.fit(train_feature, train_target)\n",
    "\n",
    "\n",
    "\n",
    "    test_size = total - train_size\n",
    "    test_feature = []\n",
    "    \n",
    "    val_sentences_present=[]\n",
    "    val_sentences_absent=[]\n",
    "    val_count=0\n",
    "    \n",
    "    val_target_answers=[]\n",
    "\n",
    "    for i in range(test_size):\n",
    "        s1 = sent1[i + train_size]\n",
    "        s2 = sent2[i + train_size]\n",
    "        can1=candidates[i+train_size][0]\n",
    "        can2=candidates[i+train_size][1]\n",
    "        c_1=candidate1[i+train_size]\n",
    "        c_2=candidate2[i+train_size]\n",
    "        try:\n",
    "            feature = extract_feature(s1, s2,can1,can2,c_1,c_2)\n",
    "            val_target_answers.append(answer[i+train_size])\n",
    "            test_feature.append(feature)\n",
    "            val_count+=1\n",
    "            val_sentences_present.append(sentences[i+train_size])\n",
    "        except:\n",
    "            val_sentences_absent.append(sentences[i+train_size])\n",
    "            \n",
    "    test_answer = clf.predict(test_feature)\n",
    "\n",
    "    print(\"Start caculating accuracy...\")\n",
    "    \n",
    "    cor=[]\n",
    "    wrong=[]\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_answer)):\n",
    "        if test_answer[i] == val_target_answers[i]:\n",
    "            correct += 1\n",
    "            cor.append(in_vocab_val[i])\n",
    "        else:\n",
    "            wrong.append(in_vocab_val[i])\n",
    "            \n",
    "    print('Accuracy of SVM:', round(float(correct) / len(test_answer) * 100,3),  \"%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n------------------------------------------------------------------\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
