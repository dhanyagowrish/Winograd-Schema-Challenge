{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import svm\n",
    "from numpy import array\n",
    "import statistics\n",
    "import csv\n",
    "from gensim.summarization import keywords\n",
    "import requests\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model with vocabulary consisting of top 400,000 word vectors\n",
    "model=KeyedVectors.load_word2vec_format(r\"\\GoogleNews-vectors-negative300.bin\",binary=True,limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a stopwords list\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.extend(['participant','nom_pronoun','occupation','$','acc_pronoun','poss_pronoun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading WinoGender dataset\n",
    "\n",
    "wGender=open('datasets\\WinoGender.tsv')\n",
    "wg=csv.reader(wGender,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining tags for different Parts-Of-Speech\n",
    "VERBS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "ADJECTIVE = ['JJ', 'JJR', 'JJS']\n",
    "NOUN=['NN','NNP','NNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return POS tagged words in sentence\n",
    "\n",
    "def make_tags(tokenized):\n",
    "    return nltk.pos_tag(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if word is a verb\n",
    "def is_verb(tup):\n",
    "    for v in VERBS:\n",
    "        if tup[1] == v:\n",
    "            return v\n",
    "    return False\n",
    "\n",
    "#check if word is a noun\n",
    "def is_noun(tup):\n",
    "    for n in NOUN:\n",
    "        if tup[1] == n:\n",
    "            return n\n",
    "    return False\n",
    "\n",
    "#check if word is an adjective\n",
    "def is_adj(tup):\n",
    "    for a in ADJECTIVE:\n",
    "        if tup[1] == a:\n",
    "            return a\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature vector for schema\n",
    "\n",
    "def extract_feature(sentence1, sentence2, candidate1, candidate2,c_1,c_2):\n",
    "\n",
    "    sent1_tags = make_tags(sentence1)\n",
    "    sent2_tags = make_tags(sentence2)\n",
    "    tags=sent1_tags+sent2_tags\n",
    "    \n",
    "    li=[]\n",
    "    li.extend(c_1.strip(\" \")+c_2.strip(\" \"))\n",
    "    \n",
    "    sentence_tokens=[word for word in tags if word[0] not in li]\n",
    "\n",
    "    sent_verb=[]\n",
    "    sent_adj=[]\n",
    "    sent_noun=[]\n",
    "    \n",
    "    for v in sentence_tokens[::-1]:\n",
    "        if is_verb(v):\n",
    "            sent_verb.append(v[0])\n",
    "            \n",
    "    for adj in sentence_tokens[::-1]:\n",
    "        if is_adj(adj):\n",
    "            sent_adj.append(adj[0])\n",
    "            \n",
    "\n",
    "    for v in sentence_tokens[::-1]:\n",
    "        if is_noun(v):\n",
    "            sent_noun.append(v[0])\n",
    "           \n",
    "    final_keywords=[]\n",
    "    \n",
    "    while(len(final_keywords)<2):\n",
    "        \n",
    "        if(len(sent_noun)>0):\n",
    "            key_noun =  sent_noun[0]\n",
    "            final_keywords.append(key_noun)\n",
    "            sent_noun.pop(0)\n",
    "        \n",
    "        if(len(sent_verb)>0):\n",
    "            key_verb  = sent_verb[0]\n",
    "            final_keywords.append(key_verb)\n",
    "            sent_verb.pop(0)\n",
    "                \n",
    "        if(len(final_keywords)>=2):\n",
    "            break\n",
    "            \n",
    "        if(len(sent_adj)>0):\n",
    "            key_adj=sent_adj[0]\n",
    "            final_keywords.append(key_adj)\n",
    "            sent_adj.pop(0)\n",
    "            \n",
    "    k_1=final_keywords[0]\n",
    "    k_2=final_keywords[1]\n",
    "  \n",
    "    ftr1 = model.similarity(candidate1, k_1)\n",
    "\n",
    "    ftr2 = model.similarity(candidate1, k_2)\n",
    "\n",
    "    ftr3 = model.similarity(candidate2, k_1)\n",
    "    \n",
    "    ftr4 = model.similarity(candidate2, k_2)\n",
    "    \n",
    "\n",
    "    return [ftr1,ftr2,ftr3,ftr4]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #list of all occupations\n",
    "    occupations=[]\n",
    "\n",
    "    #list of all participants\n",
    "    participants=[]\n",
    "\n",
    "    #tokenized_sentences\n",
    "    sentence_tokens=[]\n",
    "\n",
    "    #target answers\n",
    "    answers=[]\n",
    "\n",
    "    #whole sentences\n",
    "    sentences=[]\n",
    "    \n",
    "    # for every schema in dataset extract occupation, participant, true answer and keywords\n",
    "    for row in wg:\n",
    "        text=row[3].lower()\n",
    "        text_tokens = word_tokenize(text)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "        tokens=make_tags(tokens_without_sw)\n",
    "        sentence_tokens.append(tokens)\n",
    "        occupations.append(row[0])\n",
    "        participants.append(row[1])\n",
    "        if(row[2]=='1'):\n",
    "            answers.append(1)\n",
    "        else:\n",
    "            answers.append(-1)\n",
    "\n",
    "        text=text.replace(\"$occupation\",row[0].lower())\n",
    "        text=text.replace(\"$participant\",row[1].lower())\n",
    "        sentences.append(text)\n",
    "\n",
    "    occupations=occupations[1:]\n",
    "    participants=participants[1:]\n",
    "    answers=answers[1:]\n",
    "    sentence_tokens=sentence_tokens[1:]\n",
    "    sentences=sentences[1:]\n",
    "    \n",
    "    \n",
    "    print(\"==== WINOGENDER SCHEMAS : 120 TOTAL ... TRAIN : 96 VAL : 24====\")\n",
    "    \n",
    "    \n",
    "    print(\"Start training the model...\")\n",
    "\n",
    "    total = len(sentences)\n",
    "    train_size = 96\n",
    "    train_feature = []\n",
    "    train_target=[]\n",
    "    \n",
    "    train_count=0\n",
    "    train_sentences_present=[]\n",
    "    train_sentences_absent=[]\n",
    "    \n",
    "    \n",
    "    for i in range(train_size):\n",
    "        sent_tokens=sentence_tokens[i]\n",
    "        occ=occupations[i]\n",
    "        part=participants[i]\n",
    "        try:\n",
    "            feature = extract_feature(sent_tokens,occ,part)\n",
    "            train_target.append(answers[i])\n",
    "            train_feature.append(feature)\n",
    "            train_count+=1\n",
    "            train_sentences_present.append(sentences[i])\n",
    "\n",
    "        except:\n",
    "            train_sentences_absent.append(sentences[i])\n",
    "            \n",
    "    #SVM classifier        \n",
    "    clf = svm.SVC(gamma=\"scale\")\n",
    "    clf.fit(train_feature, train_target)\n",
    "\n",
    "\n",
    "\n",
    "    test_size = total - train_size\n",
    "    test_feature = []\n",
    "    \n",
    "    val_sentences_present=[]\n",
    "    val_sentences_absent=[]\n",
    "    val_count=0\n",
    "    \n",
    "    val_target_answers=[]\n",
    "                \n",
    "    for i in range(test_size):\n",
    "        sent_tokens=sentence_tokens[i+train_size]\n",
    "        occ=occupations[i+train_size]\n",
    "        part=participants[i+train_size]\n",
    "        try:\n",
    "            feature = extract_feature(sent_tokens,occ,part)\n",
    "            val_target_answers.append(answers[i+train_size])\n",
    "            test_feature.append(feature)\n",
    "            val_count+=1\n",
    "            val_sentences_present.append(sentences[i+train_size])\n",
    "\n",
    "        except:\n",
    "            val_sentences_absent.append(sentences[i+train_size])\n",
    "        \n",
    "    test_answer = clf.predict(test_feature)\n",
    "\n",
    "    print(\"Start caculating accuracy...\")\n",
    "    \n",
    "    val_correct_sent=[]\n",
    "    val_wrong_sent=[]\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_answer)):\n",
    "        if test_answer[i] == val_target_answers[i]:\n",
    "            correct += 1\n",
    "            val_correct_sent.append(val_sentences_present[i])\n",
    "        else:\n",
    "            val_wrong_sent.append(val_sentences_present[i])\n",
    "\n",
    "    print('Accuracy of SVM:', round(float(correct) / len(test_answer) * 100,3),  \"%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n------------------------------------------------------------------\\n\\n\")\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
